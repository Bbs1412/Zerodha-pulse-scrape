{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-info\">\n",
    "    <h2> Trying Pulse Scrapping:</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://pulse.zerodha.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying Page:\n",
    "- Pulse gives last 24 hours of news\n",
    "- Page structure (useful things only) is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<html>\n",
    "  <ul id=\"news\">                                                <!-- Has many news <li> -->\n",
    "    <li class=\"box item\">                                       <!-- News Item -->\n",
    "\n",
    "      <h2 class=\"title\">                                        <!-- News Title -->\n",
    "        <a href=\"link\">Title</a>                                <!-- Main web link -->\n",
    "      </h2>\n",
    "      <div class=\"desc\">Description</div>                       <!-- News Description -->\n",
    "      <span class=\"date\" title=\"11:32 PM, 20 Jan 2025\"></span>  <!-- News Date -->\n",
    "      <span class=\"feed\">— Bloomberg Quint</span>               <!-- News Source -->\n",
    "\n",
    "      <ul class=\"similar\">                                      <!-- Similar News (SN) -->\n",
    "        <li>\n",
    "          <a class=\"title2\" href=\"link\">News Title</a>          <!-- SN link, title -->\n",
    "          <span class=\"date\" title=\"11:32...(same)\"></span>     <!-- SN Date -->\n",
    "          <span class=\"feed\">— Bloomberg Quint</span>           <!-- SN Source -->\n",
    "          <!-- There is no description for the similar news -->\n",
    "        </li>\n",
    "        ... such more related news in li's ...\n",
    "      </ul>\n",
    "      \n",
    "    </li>\n",
    "    ... such multiple li's for news ...\n",
    "  </ul>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts = [{\n",
    "    \"title\": \"this is news headline\",\n",
    "    \"link\": \"https://www.link_to_website.com\",\n",
    "    \"description\": \"this is the description of the news\",\n",
    "    \"date\": \"09:02 PM, 21 Jan 2025\",\n",
    "    \"source\": \"website name\",\n",
    "    \"data_search\": \"{title}+{description}\",\n",
    "    \"page_content\": \"full content of actual news page (e.g. hindustan times, times of india, etc.)\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the content of the page given in link:\n",
    "page = requests.get(link)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = BeautifulSoup(page.content, 'html.parser')\n",
    "print(page_content.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fetched content in a file:\n",
    "with open(\"h_pulse_full.html\", \"w\") as file:\n",
    "    file.write(page_content.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_all = page_content.find('ul', attrs={'id': 'news'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(news_all.prettify())\n",
    "with open(\"h_all_news.html\", \"w\") as file:\n",
    "    file.write(news_all.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work on single news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_items = news_all.find_all('li', attrs={'class': 'box item'})\n",
    "print(news_items[2].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title = news_items[i].find('h2', attrs={'class': 'title'}).text\n",
    "news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the link in a tag inside this news item > h2 > a\n",
    "news_link = news_items[i].find('h2', attrs={'class': 'title'}).find('a')['href']\n",
    "news_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_desc = news_items[i].find(\"div\", attrs={\"class\": \"desc\"}).text\n",
    "news_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_date = news_items[i].find(\"span\", attrs={\"class\": \"date\"})[\"title\"]\n",
    "news_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_source = news_items[i].find(\"span\", attrs={\"class\": \"feed\"}).text\n",
    "news_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract all required things from news list-item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_content_from_li(news_list_item):\n",
    "    news_title = news_list_item.find('h2', attrs={'class': 'title'}).text\n",
    "    news_link = news_list_item.find('h2', attrs={'class': 'title'}).find('a')['href']\n",
    "    news_desc = news_list_item.find(\"div\", attrs={\"class\": \"desc\"}).text\n",
    "    news_date = news_list_item.find(\"span\", attrs={\"class\": \"date\"})[\"title\"]\n",
    "    news_source = news_list_item.find(\"span\", attrs={\"class\": \"feed\"}).text\n",
    "    news_data_search = news_title + \" \" + news_desc\n",
    "    \n",
    "    return {\n",
    "        \"title\": news_title,\n",
    "        \"link\": news_link,\n",
    "        \"description\": news_desc,\n",
    "        \"date\": news_date,\n",
    "        \"source\": news_source,\n",
    "        \"data_search\": news_data_search\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = get_news_content_from_li(news_items[2])\n",
    "print(json.dumps(resp, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified to handle cases if some data is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_content_from_li_2(news_list_item):\n",
    "    def safe_find_text(element, tag, attrs):\n",
    "        element = element.find(tag, attrs)\n",
    "        return element.text if element else \"\"\n",
    "\n",
    "    def safe_find_attr(element, tag, attrs, attr_name):\n",
    "        element = element.find(tag, attrs)\n",
    "        return element[attr_name] if element and attr_name in element.attrs else \"\"\n",
    "\n",
    "    news_title = safe_find_text(news_list_item, 'h2', {'class': 'title'})\n",
    "    news_link = safe_find_attr(news_list_item, 'a', {}, 'href')\n",
    "    news_desc = safe_find_text(news_list_item, \"div\", {\"class\": \"desc\"})\n",
    "    news_date = safe_find_attr(news_list_item, \"span\", {\"class\": \"date\"}, \"title\")\n",
    "    news_source = safe_find_text(news_list_item, \"span\", {\"class\": \"feed\"})\n",
    "    news_data_search = f\"{news_title} {news_desc}\".strip()\n",
    "\n",
    "    return {\n",
    "        \"title\": news_title,\n",
    "        \"link\": news_link,\n",
    "        \"description\": news_desc,\n",
    "        \"date\": news_date,\n",
    "        \"source\": news_source,\n",
    "        \"data_search\": news_data_search\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp2 = get_news_content_from_li_2(news_items[2])\n",
    "print(json.dumps(resp, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic to handle related news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_news_content_from_li(similar_news_list_item):\n",
    "    def safe_find_text(element, tag, attrs):\n",
    "        element = element.find(tag, attrs)\n",
    "        return element.text if element else \"\"\n",
    "\n",
    "    def safe_find_attr(element, tag, attrs, attr_name):\n",
    "        element = element.find(tag, attrs)\n",
    "        return element[attr_name] if element and attr_name in element.attrs else \"\"\n",
    "\n",
    "    news_title = safe_find_text(similar_news_list_item, 'a', {'class': 'title2'})\n",
    "    news_link = safe_find_attr(similar_news_list_item, 'a', {'class': 'title2'}, 'href')\n",
    "    news_desc = safe_find_text(similar_news_list_item, \"div\", {\"class\": \"desc\"})\n",
    "    news_date = safe_find_attr(similar_news_list_item, \"span\", {\"class\": \"date\"}, \"title\")\n",
    "    news_source = safe_find_text(similar_news_list_item, \"span\", {\"class\": \"feed\"})\n",
    "    news_data_search = f\"{news_title} {news_desc}\".strip()\n",
    "\n",
    "    return {\n",
    "        \"title\": news_title,\n",
    "        \"link\": news_link,\n",
    "        \"description\": news_desc,\n",
    "        \"date\": news_date,\n",
    "        \"source\": news_source,\n",
    "        \"data_search\": news_data_search\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 7\n",
    "news_items[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if news_items[i].find(\"ul\", attrs={\"class\": \"similar\"}):\n",
    "    print(\"Similar news exist. Appending it as separate news item\")\n",
    "\n",
    "    similar_news = news_items[i].find(\"ul\", attrs={\"class\": \"similar\"}).find_all(\"li\")\n",
    "    # print(similar_news[0].prettify())\n",
    "    for news in similar_news:\n",
    "        resp = get_similar_news_content_from_li(news)\n",
    "        print(json.dumps(resp, indent=4))\n",
    "else:\n",
    "    print(\"similar news not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Final Code:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See the structure study and output format before this code.\n",
    "- The related and main news are both combined in the same list.\n",
    "- Two functions are merged into single function with related_news flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import argparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from bs4.element import Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add argument parser to take keyword as input from cmd:\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--keyword\", help=\"Keyword to search in news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for jupyter notebook:\n",
    "keyword = \"adani\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take keyword as input from cmd as --keyword zomato:\n",
    "args = parser.parse_args()\n",
    "if args.keyword:\n",
    "    keyword = args.keyword\n",
    "else:\n",
    "    print(\"No keyword provided. Using default keyword 'zomato'\")\n",
    "    keyword = \"zomato\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://pulse.zerodha.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get page:\n",
    "page = requests.get(link)\n",
    "\n",
    "if page.status_code != 200:\n",
    "    raise Exception(f\"Failed to fetch page. Status code: {page.status_code}\")\n",
    "else:\n",
    "    print(\"Page fetched successfully...\")\n",
    "    \n",
    "page_content = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(filename_safe=True, spaces=False):\n",
    "    if filename_safe and spaces:\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %I-%M-%S %p\")\n",
    "    if filename_safe and not spaces:\n",
    "        return datetime.now().strftime(\"%Y-%m-%d_%I-%M-%S_%p\")\n",
    "\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")\n",
    "\n",
    "# timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fetched content in a file:\n",
    "filename = f\"./{timestamp()}_full.html\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(page_content.prettify())\n",
    "print(f'Page content saved to file: \"{filename}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the news list:\n",
    "news_all = page_content.find('ul', attrs={'id': 'news'})\n",
    "news_items = news_all.find_all('li', attrs={'class': 'box item'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get news content from a news list item:\n",
    "def get_news_content(news_list_item, is_similar_news=False):\n",
    "    def safe_find_text(element, tag, attrs):\n",
    "        element = element.find(tag, attrs)\n",
    "        return element.text if element else \"\"\n",
    "\n",
    "    def safe_find_attr(element, tag, attrs, attr_name):\n",
    "        element = element.find(tag, attrs)\n",
    "        return element[attr_name] if element and attr_name in element.attrs else \"\"\n",
    "\n",
    "\n",
    "    if not is_similar_news:\n",
    "        news_title = safe_find_text(news_list_item, 'h2', {'class': 'title'})\n",
    "        news_link = safe_find_attr(news_list_item, 'a', {}, 'href')\n",
    "        news_desc = safe_find_text(news_list_item, \"div\", {\"class\": \"desc\"})\n",
    "        \n",
    "    else:\n",
    "        news_title = safe_find_text(news_list_item, 'a', {'class': 'title2'})\n",
    "        news_link = safe_find_attr(news_list_item, 'a', {'class': 'title2'}, 'href')\n",
    "        news_desc = \"\" # No description available for similar news\n",
    "        \n",
    "    news_date = safe_find_attr(news_list_item, \"span\", {\"class\": \"date\"}, \"title\")\n",
    "    news_source = safe_find_text(news_list_item, \"span\", {\"class\": \"feed\"})\n",
    "    news_data_search = f\"{news_title} {news_desc}\".strip()\n",
    "\n",
    "    return {\n",
    "        \"title\": news_title,\n",
    "        \"link\": news_link,\n",
    "        \"description\": news_desc,\n",
    "        \"date\": news_date,\n",
    "        \"source\": news_source,\n",
    "        \"data_search\": news_data_search\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_news_list = []\n",
    "\n",
    "for news in news_items:\n",
    "    # print(json.dumps(get_news_content(news), indent=4))\n",
    "    final_news_list.append(get_news_content(news))\n",
    "    \n",
    "    # Check if similar news exist, if yes, append them as separate news items:\n",
    "    if news.find(\"ul\", attrs={\"class\": \"similar\"}):\n",
    "        # print(\"Similar news exist. Appending it as separate news item\")\n",
    "        similar_news = news.find(\"ul\", attrs={\"class\": \"similar\"}).find_all(\"li\")\n",
    "\n",
    "        for similar_news_item in similar_news:\n",
    "            # print(json.dumps(get_news_content(similar_news_item, is_similar_news=True), indent=4))\n",
    "            final_news_list.append(get_news_content(similar_news_item, is_similar_news=True))\n",
    "\n",
    "print(\"\\n\\nProcessing completed successfully...\")\n",
    "print(\"Total main news scraped :\".rjust(40), len(news_items))\n",
    "print(\"Total related news scraped :\".rjust(40), len(final_news_list) - len(news_items))\n",
    "print(\"Total news scraped :\".rjust(40), len(final_news_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final news list in a file:\n",
    "filename = f\"./{timestamp()}_news.json\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(final_news_list, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query News:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find for the keyword in the news list:\n",
    "keyword = keyword.lower()\n",
    "keyword_news = []\n",
    "\n",
    "# Find the keyword in the news list:\n",
    "for news in final_news_list:\n",
    "    if keyword in news['data_search'].lower():\n",
    "        keyword_news.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total news items with keyword :\".rjust(40), len(keyword_news), end=\"\\n\\n\")\n",
    "print(f'Saved all news in file: \"{filename}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the keyword news list in a file:\n",
    "filename = f\"./{timestamp()}_query.json\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(keyword_news, file, indent=4)\n",
    "print(f'Saved queried news in file: \"{filename}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(string=True)\n",
    "    visible_texts = filter(tag_visible, texts)\n",
    "\n",
    "    # Return the non-empty visible texts joined by new line:\n",
    "    final_text = \"\\n\".join(t.strip() for t in visible_texts if t.strip() != \"\")\n",
    "    # final_text = u\"\\n\\n\".join(t.strip() for t in visible_texts)\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives only the news content from entire page:\n",
    "# Uses custom code to remove unwanted tags and text\n",
    "# Coded as per the html structures of various news sources\n",
    "\n",
    "def handle_source_wise_news(resp, link) -> str:\n",
    "    page_content = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    if page_content is None:\n",
    "        return \"Could not fetch page content\"\n",
    "\n",
    "\n",
    "    if \"economictimes\" in link.lower():\n",
    "        print(\"\\tStep   :> ET article > \", sep=\"\", end=\"\")\n",
    "        # find div with class=\"artText medium\"\n",
    "        news_div = page_content.find(\n",
    "            \"div\", attrs={\"class\": [\"artText\", \"medium\"]})\n",
    "\n",
    "        # If div in this pattern is not found, then return full page content:\n",
    "        # Bcz ET has used different pattern for some news articles\n",
    "        if news_div is None:\n",
    "            print(\"Diff html pattern > Return full page.\")\n",
    "            return text_from_html(page_content.get_text())\n",
    "\n",
    "        # if news_div found, delete any tag with classes \"adBox\" or \"custom_ad\"\n",
    "        for ad in page_content.find_all(attrs={\"class\": [\"adBox\", \"custom_ad\"]}):\n",
    "            ad.decompose()\n",
    "        print(\"Removed ads > Return News.\")\n",
    "\n",
    "        return text_from_html(news_div.prettify())\n",
    "\n",
    "\n",
    "    elif \"thehindu\" in link.lower():\n",
    "        print(\"\\tStep   :> The-Hindu article > \", sep=\"\", end=\"\")\n",
    "        # div with class \"storyline\" has news content\n",
    "        # real news (without header and all) is div.articlebodycontent\n",
    "        news_div = page_content.find(\"div\", attrs={\"class\": \"storyline\"})\n",
    "\n",
    "        # If its not found, return full page content (assuming diff html pattern):\n",
    "        if news_div is None:\n",
    "            print(\"Diff html pattern > Return full page.\")\n",
    "            return text_from_html(page_content.get_text())\n",
    "\n",
    "        # If news found: div.artinrstl-ad div.article-ad div.artmeterpv div.also-read\n",
    "        for ad in news_div.find_all(attrs={\"class\": [\"artinrstl-ad\", \"article-ad\", \"artmeterpv\", \"also-read\", \"share-page\"]}):\n",
    "            ad.decompose()\n",
    "        print(\"Removed ads > \", sep=\"\", end=\"\")\n",
    "\n",
    "        # Rm all content after div.article-published-time-end\n",
    "        cutoff_tag = news_div.find(\n",
    "            \"div\", attrs={\"class\": \"article-published-time-end\"})\n",
    "        if cutoff_tag:\n",
    "            for sibling in list(cutoff_tag.find_all_next()):\n",
    "                sibling.decompose()\n",
    "            print(\"Removed trailing content > \", sep=\"\", end=\"\")\n",
    "\n",
    "        print(\"Return News.\")\n",
    "        return text_from_html(news_div.prettify())\n",
    "\n",
    "\n",
    "    elif \"ndtvprofit\" in link.lower():\n",
    "        # This seems complex one, randomly generated ids and classes\n",
    "        # Approach 1:\n",
    "        # # main news in div.\"story-base-template-m__body-container__ZeMOl\"\n",
    "        # # ads: div.story-element-text-also-read responsive-ad\n",
    "        # Approach 2:\n",
    "        # find all div.\"story-element story-element-text\"\n",
    "        # remove ads from these ones using same ad pattern\n",
    "\n",
    "        print(\"\\tStep   :> NDTV Profit article > \", sep=\"\", end=\"\")\n",
    "        # Approach 2: find all div.\"story-element story-element-text\"\n",
    "        news_divs = page_content.find_all(\n",
    "            \"div\", attrs={\"class\": [\"story-element\", \"story-element-text\", \"key-highlights-wrapper\"]})\n",
    "        if not news_divs:\n",
    "            print(\"Diff html pattern > Return full page.\")\n",
    "            return text_from_html(page_content.get_text())\n",
    "\n",
    "        # If news found: delete div.\"story-element-text-also-read responsive-ad\"\n",
    "        for news_div in news_divs:\n",
    "            # for ad in news_div.find_all(attrs={\"class\": [\"story-element-text-also-read\", \"responsive-ad\"]}):\n",
    "            for ad in news_div.find_all(\"div\", attrs={\"class\": [\"story-element-text-also-read\", \"responsive-ad\", \"also-read-container\", \"also-read-m__bb-opinion-container__V-vnP\"]}):\n",
    "                ad.decompose()\n",
    "\n",
    "        print(\"Removed ads > Return News.\")\n",
    "        # put all news divs together and return\n",
    "        dummy_page = f\"<body> {''.join([news_div.prettify() for news_div in news_divs])} </body>\"\n",
    "        return text_from_html(dummy_page)\n",
    "\n",
    "    # elif \"moneycontrol\" in link.lower():\n",
    "\n",
    "    # For any other source, return full page content:\n",
    "    else:\n",
    "        return text_from_html(page_content.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm:\n",
    "- The Hindu, NDTV Profit (Bloomberg Quint), and Economic Times are checked.\n",
    "- ZeeBiz, MoneyControl, and ?? are not coded and checked.\n",
    "\n",
    "### Test source wise:\n",
    "- Do not delete this 👇 cell\n",
    "- it is for testing purpose\n",
    "- Instead, comment it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link = \"https://economictimes.indiatimes.com/markets/expert-view/food-delivery-slowdown-hits-zomato-but-qsr-resilience-could-lead-to-recovery-jignanshu-gor/articleshow/117451126.cms\"\n",
    "# link = \"https://www.thehindu.com/business/budget/budget-should-announce-tax-cuts-for-individuals-to-boost-consumption-barclays/article69131051.ece\"\n",
    "# link = \"https://www.ndtvprofit.com/quarterly-earnings/q3-results-2025-today-adani-green-adani-energy-solutions-dr-reddys-lab-hpcl-indus-towers-quarterly-earnings-live-blog\"\n",
    "\n",
    "# resp = requests.get(link)\n",
    "# news_content = handle_source_wise_news(resp, link)\n",
    "# with open(\"temp.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     json.dump({\"news\": news_content}, file, indent=4)\n",
    "# print(news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_article(link) -> tuple[bool, str]:\n",
    "    try:\n",
    "        resp = requests.get(link)\n",
    "\n",
    "        if resp.status_code != 200:\n",
    "            flag = False\n",
    "            page_content = \"\"\n",
    "        else:\n",
    "            if resp.content is None:\n",
    "                flag = False\n",
    "                page_content = \"No content found\"\n",
    "            else:\n",
    "                flag = True\n",
    "                page_content = handle_source_wise_news(resp, link)\n",
    "            # print(page_content)\n",
    "    except Exception as e:\n",
    "        flag = False\n",
    "        page_content = \"Error: \" + e\n",
    "    \n",
    "    finally:\n",
    "        return flag, page_content\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the individual ones:\n",
    "print()\n",
    "total, success, fail = len(keyword_news), 0, 0\n",
    "\n",
    "def wrap(text, offset=15):\n",
    "    # 15 is calc from \"\\tLink   :> \" length + 1\n",
    "    t_width = shutil.get_terminal_size().columns\n",
    "    # IDK why but sometimes, text still overflows, so -10 to be safe\n",
    "    rem = t_width - offset - 10\n",
    "    if len(text) > rem:\n",
    "        return text[: rem] + \"...\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for news in keyword_news:\n",
    "    print(f\"\\n[{success + fail + 1:02d} / {total:02d}]\")\n",
    "    print(f\"\\tNews   :> { wrap(news['title']) }\")\n",
    "    print(f\"\\tFrom   :> { wrap(news['source']) }\")\n",
    "    # print(f\"\\tLink   :> {news['link']}\")\n",
    "    print(f\"\\tLink   :> { wrap(news['link']) }\")\n",
    "\n",
    "    flag, content = get_news_article(news['link'])\n",
    "    if flag:\n",
    "        print(f\"\\tStatus :> ✅ Success...\")\n",
    "        success += 1\n",
    "        news['full_news'] = content\n",
    "    else:\n",
    "        print(f\"\\tStatus :> ❌ Failed...\")\n",
    "        fail += 1\n",
    "        news['full_news'] = \"Failed to fetch news content...\"\n",
    "\n",
    "    # break\n",
    "\n",
    "print()\n",
    "print(\"Queried total :\".rjust(40), total)\n",
    "print(\"Successful :\".rjust(40), success)\n",
    "print(\"Failed :\".rjust(40), fail)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the keyword news list in a file:\n",
    "filename = f\"./{timestamp()}_query_final.json\"\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(keyword_news, file, indent=4)\n",
    "print(f'Saved final query results in file: \"{filename}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scrap-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
